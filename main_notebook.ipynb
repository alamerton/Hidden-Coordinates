{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8CLAMfSJ2Av"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYa2vunsJ2Aw",
        "outputId": "0df96066-50f0-488c-b062-941b4bbce316"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '../requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4nrbB-4J2Aw"
      },
      "source": [
        "Import and manage libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgUAv4pDJ2Aw",
        "outputId": "645535c7-fb99-4f7c-d953-f1316bd500df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x17fb83e90>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import einops\n",
        "from ivy import to_numpy\n",
        "import plotly_express as px\n",
        "\n",
        "# Enable gradient calculations (by default)\n",
        "\n",
        "torch.set_grad_enabled(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E1kLhZeJ2Ax"
      },
      "source": [
        "Load GPT-2 small model from transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwqTHBvoJ2Ax",
        "outputId": "b64aa218-b673-4d4b-c01d-49b46fde8d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSWV0qB2J2Ax"
      },
      "source": [
        "Initialise model components as variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yplTYcBJ2Ax"
      },
      "outputs": [],
      "source": [
        "n_layers = model.cfg.n_layers  # Number of transformer layers\n",
        "d_model = model.cfg.d_model    # Dimension of the model\n",
        "n_heads = model.cfg.n_heads    # Number of attention heads\n",
        "d_head = model.cfg.d_head      # Dimension of each attention head\n",
        "d_mlp = model.cfg.d_mlp        # Dimension of the MLP (Feed Forward network) within the transformer\n",
        "d_vocab = model.cfg.d_vocab    # Size of the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKfSwyiiJ2Ax"
      },
      "source": [
        "Load and display common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v09Le_NMJ2Ax",
        "outputId": "38919528-2020-466f-8d95-71bec8a03f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'aa', 'aaa', 'aaron', 'ab', 'abandoned', 'abc', 'aberdeen', 'abilities', 'ability']\n"
          ]
        }
      ],
      "source": [
        "common_words = open(\"common_words.txt\", \"r\").read().split(\"\\n\")\n",
        "print(common_words[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwhzHUiAJ2Ax"
      },
      "source": [
        "Calculate the number of tokens for each common word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWOxW-FxJ2Ax"
      },
      "outputs": [],
      "source": [
        "num_tokens = [len(model.to_tokens(\" \" + word, prepend_bos=False).squeeze(0)) for word in common_words]\n",
        "print(list(zip(num_tokens, common_words))[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF7zbL32J2Ax"
      },
      "source": [
        "Create a DataFrame of words and their token counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8l2zOKUJ2Ay"
      },
      "outputs": [],
      "source": [
        "word_df = pd.DataFrame({\"word\": common_words, \"num_tokens\": num_tokens})\n",
        "word_df = word_df.query('num_tokens < 4')  # Filter words with less than 4 tokens\n",
        "word_df.value_counts(\"num_tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJW4utzgJ2Ay"
      },
      "source": [
        "Define the prefix for context and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4iKXH_KJ2Ay"
      },
      "outputs": [],
      "source": [
        "prefix = \"The United States Declaration of Independence received its first formal public reading, in Philadelphia.\\nWhen\"\n",
        "PREFIX_LENGTH = len(model.to_tokens(prefix, prepend_bos=True).squeeze(0))\n",
        "NUM_WORDS = 7\n",
        "MAX_WORD_LENGTH = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbHdNdItJ2Ay"
      },
      "source": [
        "Split the data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGDzTaaRJ2Ay"
      },
      "outputs": [],
      "source": [
        "train_filter = np.random.rand(len(word_df)) < 0.8\n",
        "train_word_df = word_df.iloc[train_filter]\n",
        "test_word_df = word_df.iloc[~train_filter]\n",
        "print(train_word_df.value_counts(\"num_tokens\"))\n",
        "print(test_word_df.value_counts(\"num_tokens\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nUxYJbuJ2Ay"
      },
      "source": [
        "Group words by their token length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy8mXGcEJ2Ay"
      },
      "outputs": [],
      "source": [
        "train_word_by_length_array = [np.array([\" \" + j for j in train_word_df[train_word_df.num_tokens == i].word.values]) for i in range(1, MAX_WORD_LENGTH + 1)]\n",
        "test_word_by_length_array = [np.array([\" \" + j for j in test_word_df[test_word_df.num_tokens == i].word.values]) for i in range(1, MAX_WORD_LENGTH + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7kWVufAJ2Ay"
      },
      "source": [
        "Define a function to generate batches of tokenized inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fr3OJ3CJ2Ay"
      },
      "outputs": [],
      "source": [
        "def gen_batch(batch_size, word_by_length_array):\n",
        "    word_lengths = torch.randint(1, MAX_WORD_LENGTH+1, (batch_size, NUM_WORDS))\n",
        "    words = []\n",
        "    for i in range(batch_size):\n",
        "        row = []\n",
        "        for word_len in word_lengths[i].tolist():\n",
        "            word = word_by_length_array[word_len-1][np.random.randint(len(word_by_length_array[word_len-1]))]\n",
        "            row.append(word)\n",
        "        words.append(\"\".join(row))\n",
        "    full_tokens = torch.ones((batch_size, PREFIX_LENGTH + MAX_WORD_LENGTH*NUM_WORDS), dtype=torch.int64)\n",
        "    tokens = model.to_tokens([prefix + word for word in words], prepend_bos=True)\n",
        "    full_tokens[:, :tokens.shape[-1]] = tokens\n",
        "\n",
        "    first_token_indices = torch.concatenate([\n",
        "        torch.zeros(batch_size, dtype=int)[:, None], word_lengths.cumsum(dim=-1)[..., :-1]\n",
        "    ], dim=-1) + PREFIX_LENGTH\n",
        "\n",
        "    last_token_indices = word_lengths.cumsum(dim=-1) - 1 + PREFIX_LENGTH\n",
        "    return full_tokens, words, word_lengths, first_token_indices, last_token_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-1y7WXWJ2Ay"
      },
      "source": [
        "Generate a batch of tokens and their related information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev_4zTnxJ2Ay"
      },
      "outputs": [],
      "source": [
        "tokens, words, word_lengths, first_token_indices, last_token_indices = gen_batch(10, train_word_by_length_array)\n",
        "tokens, words, word_lengths, first_token_indices, last_token_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKVaOtR_J2Ay"
      },
      "source": [
        "Set training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN0n495ZJ2Ay"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "epochs = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5TNPAR0J2Ay"
      },
      "source": [
        "Collect residuals for tokens across multiple epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ait2QT3fJ2Ay"
      },
      "outputs": [],
      "source": [
        "torch.set_grad_enabled(False)\n",
        "epochs = 100\n",
        "all_first_token_residuals = []\n",
        "all_last_token_residuals = []\n",
        "\n",
        "for i in tqdm.tqdm(range(epochs)):\n",
        "    tokens, words, word_lengths, first_token_indices, last_token_indices = gen_batch(batch_size, train_word_by_length_array)\n",
        "    with torch.no_grad():\n",
        "        # _, cache = model.run_with_cache(tokens.cuda(), names_filter=lambda x: x.endswith(\"resid_post\"))\n",
        "        _, cache = model.run_with_cache(tokens, names_filter=lambda x: x.endswith(\"resid_post\")) # Can't run run_with_cache with CUDA on my Mac, just passing 'tokens' instead\n",
        "        residuals = cache.stack_activation(\"resid_post\")\n",
        "        first_token_residuals = residuals[:, torch.arange(len(first_token_indices)).to(residuals.device)[:, None], first_token_indices, :]\n",
        "        last_token_residuals = residuals[:, torch.arange(len(last_token_indices)).to(residuals.device)[:, None], last_token_indices, :]\n",
        "        print(\"Shapes\", first_token_residuals.shape, last_token_residuals.shape)\n",
        "        all_first_token_residuals.append(to_numpy(first_token_residuals))\n",
        "        all_last_token_residuals.append(to_numpy(last_token_residuals))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvQIFLlRJ2Ay"
      },
      "source": [
        "#### Run from here to avoid repeating 100 epoch runs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG4Ly0rEJ2Ay"
      },
      "source": [
        "Data preparation for training a Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPsvqbkUJ2Ay"
      },
      "outputs": [],
      "source": [
        "LAYER = 0\n",
        "y = np.array([j for i in range(len(all_first_token_residuals[0])) for j in range(NUM_WORDS)])\n",
        "layer_data = all_last_token_residuals[LAYER]\n",
        "X = layer_data[:, :].reshape(-1, d_model)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "\n",
        "x_indices = to_numpy(torch.randperm(len(X))[:10000])\n",
        "y_indices = to_numpy(torch.randperm(len(y))[:10000])\n",
        "common_indices = np.intersect1d(x_indices, y_indices)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[common_indices], y[common_indices], test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdOjZojFJ2Ay"
      },
      "source": [
        "Create and train a Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZwtsAgAJ2Az"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression(multi_class='ovr', solver='saga', random_state=42, max_iter=100, C=1.0)\n",
        "lr_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z3e1CzFJ2Az"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WOPVtEVJ2Az"
      },
      "outputs": [],
      "source": [
        "y_pred = lr_model.predict(X_train)\n",
        "print(classification_report(y_train, y_pred))\n",
        "y_pred = lr_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sybeJlPQJ2Az"
      },
      "source": [
        "Generate predictions on test batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQkwtBoiJ2Az"
      },
      "outputs": [],
      "source": [
        "test_batches = 10\n",
        "last_token_predictions_list = []\n",
        "last_token_abs_indices_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(range(test_batches)):\n",
        "        tokens, words, word_lengths, first_token_indices, last_token_indices = gen_batch(batch_size, test_word_by_length_array)\n",
        "        _, cache = model.run_with_cache(tokens, names_filter=lambda x: x.endswith(\"resid_post\"))\n",
        "        residuals = cache.stack_activation(\"resid_post\")\n",
        "        first_token_residuals = residuals[:, torch.arange(len(first_token_indices)).to(residuals.device)[:, None], first_token_indices, :]\n",
        "        last_token_residuals = residuals[:, torch.arange(len(last_token_indices)).to(residuals.device)[:, None], last_token_indices, :]\n",
        "        last_token_resids = to_numpy(einops.rearrange(last_token_residuals[LAYER], \"batch word d_model -> (batch word) d_model\"))\n",
        "        last_token_predictions_list.append(lr_model.predict(last_token_resids))\n",
        "        last_token_abs_indices_list.append(to_numpy(last_token_indices.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiRJYsK8J2Az"
      },
      "source": [
        "Prepare and visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bcambufJ2Az"
      },
      "outputs": [],
      "source": [
        "last_token_abs_indices = np.concatenate(last_token_abs_indices_list)\n",
        "last_token_predictions = np.concatenate(last_token_predictions_list)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"index\": [i for _ in range(batch_size * test_batches) for i in range(NUM_WORDS)],\n",
        "    \"abs_pos\": last_token_abs_indices,\n",
        "    \"pred\": last_token_predictions\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEgyhDM4J2Az"
      },
      "source": [
        "Plot histogram of the prediction results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swTWYMmwJ2Az"
      },
      "outputs": [],
      "source": [
        "px.histogram(df, x=\"abs_pos\", color=\"pred\", facet_row=\"index\", barnorm=\"fraction\").show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hidden-coordinates",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}