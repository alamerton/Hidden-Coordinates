{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '../requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and manage libraries and packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x17fb83e90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import einops\n",
    "from ivy import to_numpy\n",
    "import plotly_express as px\n",
    "\n",
    "# Enable gradient calculations (by default)\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GPT-2 small model from transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model components as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers  # Number of transformer layers\n",
    "d_model = model.cfg.d_model    # Dimension of the model\n",
    "n_heads = model.cfg.n_heads    # Number of attention heads\n",
    "d_head = model.cfg.d_head      # Dimension of each attention head\n",
    "d_mlp = model.cfg.d_mlp        # Dimension of the MLP (Feed Forward network) within the transformer\n",
    "d_vocab = model.cfg.d_vocab    # Size of the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and display common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../common_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m common_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../common_words.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(common_words[:\u001b[38;5;241m10\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hidden-coordinates/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../common_words.txt'"
     ]
    }
   ],
   "source": [
    "common_words = open(\"../common_words.txt\", \"r\").read().split(\"\\n\")\n",
    "print(common_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of tokens for each common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(model.to_tokens(\" \" + word, prepend_bos=False).squeeze(0)) for word in common_words]\n",
    "print(list(zip(num_tokens, common_words))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame of words and their token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame({\"word\": common_words, \"num_tokens\": num_tokens})\n",
    "word_df = word_df.query('num_tokens < 4')  # Filter words with less than 4 tokens\n",
    "word_df.value_counts(\"num_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prefix for context and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"The United States Declaration of Independence received its first formal public reading, in Philadelphia.\\nWhen\"\n",
    "PREFIX_LENGTH = len(model.to_tokens(prefix, prepend_bos=True).squeeze(0))\n",
    "NUM_WORDS = 7\n",
    "MAX_WORD_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.random.rand(len(word_df)) < 0.8\n",
    "train_word_df = word_df.iloc[train_filter]\n",
    "test_word_df = word_df.iloc[~train_filter]\n",
    "print(train_word_df.value_counts(\"num_tokens\"))\n",
    "print(test_word_df.value_counts(\"num_tokens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group words by their token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_by_length_array = [np.array([\" \" + j for j in train_word_df[train_word_df.num_tokens == i].word.values]) for i in range(1, MAX_WORD_LENGTH + 1)]\n",
    "test_word_by_length_array = [np.array([\" \" + j for j in test_word_df[test_word_df.num_tokens == i].word.values]) for i in range(1, MAX_WORD_LENGTH + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate batches of tokenized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(batch_size, word_by_length_array):\n",
    "    word_lengths = torch.randint(1, MAX_WORD_LENGTH+1, (batch_size, NUM_WORDS))\n",
    "    words = []\n",
    "    for i in range(batch_size):\n",
    "        row = []\n",
    "        for word_len in word_lengths[i].tolist():\n",
    "            word = word_by_length_array[word_len-1][np.random.randint(len(word_by_length_array[word_len-1]))]\n",
    "            row.append(word)\n",
    "        words.append(\"\".join(row))\n",
    "    full_tokens = torch.ones((batch_size, PREFIX_LENGTH + MAX_WORD_LENGTH*NUM_WORDS), dtype=torch.int64)\n",
    "    tokens = model.to_tokens([prefix + word for word in words], prepend_bos=True)\n",
    "    full_tokens[:, :tokens.shape[-1]] = tokens\n",
    "    \n",
    "    first_token_indices = torch.concatenate([\n",
    "        torch.zeros(batch_size, dtype=int)[:, None], word_lengths.cumsum(dim=-1)[..., :-1]\n",
    "    ], dim=-1) + PREFIX_LENGTH\n",
    "    \n",
    "    last_token_indices = word_lengths.cumsum(dim=-1) - 1 + PREFIX_LENGTH\n",
    "    return full_tokens, words, word_lengths, first_token_indices, last_token_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a batch of tokens and their related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, words, word_lengths, first_token_indices, last_token_indices = gen_batch(10, train_word_by_length_array)\n",
    "tokens, words, word_lengths, first_token_indices, last_token_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect residuals for tokens across multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "epochs = 100\n",
    "all_first_token_residuals = []\n",
    "all_last_token_residuals = []\n",
    "                                                        \n",
    "for i in tqdm.tqdm(range(epochs)):\n",
    "    tokens, words, word_lengths, first_token_indices, last_token_indices = gen_batch(batch_size, train_word_by_length_array)\n",
    "    with torch.no_grad():\n",
    "        # _, cache = model.run_with_cache(tokens.cuda(), names_filter=lambda x: x.endswith(\"resid_post\"))\n",
    "        _, cache = model.run_with_cache(tokens, names_filter=lambda x: x.endswith(\"resid_post\")) # Can't run run_with_cache with CUDA on my Mac, just passing 'tokens' instead \n",
    "        residuals = cache.stack_activation(\"resid_post\")\n",
    "        first_token_residuals = residuals[:, torch.arange(len(first_token_indices)).to(residuals.device)[:, None], first_token_indices, :]\n",
    "        last_token_residuals = residuals[:, torch.arange(len(last_token_indices)).to(residuals.device)[:, None], last_token_indices, :]\n",
    "        print(\"Shapes\", first_token_residuals.shape, last_token_residuals.shape)\n",
    "        all_first_token_residuals.append(to_numpy(first_token_residuals))\n",
    "        all_last_token_residuals.append(to_numpy(last_token_residuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run from here to avoid repeating 100 epoch runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation for training a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 0\n",
    "y = np.array([j for i in range(len(all_first_token_residuals[0])) for j in range(NUM_WORDS)])\n",
    "layer_data = all_last_token_residuals[LAYER]\n",
    "X = layer_data[:, :].reshape(-1, d_model)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "\n",
    "x_indices = to_numpy(torch.randperm(len(X))[:10000])\n",
    "y_indices = to_numpy(torch.randperm(len(y))[:10000])\n",
    "common_indices = np.intersect1d(x_indices, y_indices)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[common_indices], y[common_indices], test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(multi_class='ovr', solver='saga', random_state=42, max_iter=100, C=1.0)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))\n",
    "y_pred = lr_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions on test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = 10\n",
    "last_token_predictions_list = []\n",
    "last_token_abs_indices_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm.tqdm(range(test_batches)):\n",
    "        tokens, words, word_lengths, first_token_indices, last_token_indices = gen_batch(batch_size, test_word_by_length_array)\n",
    "        _, cache = model.run_with_cache(tokens, names_filter=lambda x: x.endswith(\"resid_post\"))\n",
    "        residuals = cache.stack_activation(\"resid_post\")\n",
    "        first_token_residuals = residuals[:, torch.arange(len(first_token_indices)).to(residuals.device)[:, None], first_token_indices, :]\n",
    "        last_token_residuals = residuals[:, torch.arange(len(last_token_indices)).to(residuals.device)[:, None], last_token_indices, :]\n",
    "        last_token_resids = to_numpy(einops.rearrange(last_token_residuals[LAYER], \"batch word d_model -> (batch word) d_model\"))\n",
    "        last_token_predictions_list.append(lr_model.predict(last_token_resids))\n",
    "        last_token_abs_indices_list.append(to_numpy(last_token_indices.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_abs_indices = np.concatenate(last_token_abs_indices_list)\n",
    "last_token_predictions = np.concatenate(last_token_predictions_list)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"index\": [i for _ in range(batch_size * test_batches) for i in range(NUM_WORDS)],\n",
    "    \"abs_pos\": last_token_abs_indices,\n",
    "    \"pred\": last_token_predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"abs_pos\", color=\"pred\", facet_row=\"index\", barnorm=\"fraction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hidden-coordinates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
